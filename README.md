# Animal Species Prediction (MobileNetV2)

Comprehensive documentation for the `animal-species-prediction` project.

This repository contains code and assets for a MobileNetV2-based image classifier (animals) with an interactive Streamlit dashboard that supports image and video inference, Grad-CAM localization, and annotated outputs.

----

**Contents**
- **Project overview** — short description and goals
- **Repo layout** — file & folder descriptions
- **How it works** — detailed explanations for each component (model loading, Grad-CAM, predict pipeline, video processing, dashboard)
- **How to run** — environment setup, commands
- **Training & evaluation** — how to train, evaluate, and save metrics
- **Outputs & formats** — where annotated outputs are stored and what to expect
- **Troubleshooting & tips**
- **Next steps & improvements**

----

**Project overview**

This project demonstrates a classifier trained on animal images (classes such as `bird`, `cat`, `deer`, `dog`, `frog`, `horse`). The model is MobileNetV2-based (PyTorch) and used for two primary workflows:

- Image inference with Grad-CAM localization (save annotated images and optional crops).
- Frame-by-frame video annotation (apply same CAM localization per frame and save an annotated MP4).

An interactive Streamlit dashboard (`dashboard.py`) provides upload + inference for images and videos, probability visualization for images, and a downloadable annotated video after video processing.

----

**Repository layout (important files)**

Root
- `README.md` (this file)
- `requirements.txt` — Python packages used by the project
- `run.bat`, `run.sh` — convenience run scripts
- `data/animals/` — dataset used for training/validation (organized into `train/` and `val/` with one folder per class)
- `models/` — model weights and saved metrics (e.g., `best_model.pt`, optional `metrics.npy`)
- `outputs/` — annotated outputs generated by the predict / video pipelines
- `src/` — project source code
  - `dataset.py` — dataset utilities and PyTorch Dataset/loader
  - `train.py` — training script for training MobileNetV2 on the animals dataset
  - `predict.py` — image inference + Grad-CAM + annotated image saving
  - `video_predict.py` — video frame annotation using the same model and CAM
  - `__pycache__/` — compiled caches
- `scripts/` — small helper scripts (e.g., `print_model_summary.py`, evaluation helpers)
- `dashboard.py` — Streamlit dashboard (image & video upload UI + inference)

----

**How each component works (detailed)**

1) Model loading (`src.predict.load_model`)

- This function loads a MobileNetV2 architecture and the best weights (expected at `models/best_model.pt`).
- It returns `(model, classes, img_size)`:
  - `model`: PyTorch nn.Module in eval mode (moved to CPU/GPU at call site)
  - `classes`: list of class names in training order (used for mapping indices to labels)
  - `img_size`: input image size expected by the model (e.g., 224)

If `best_model.pt` is not found the code may either create a fresh architecture or raise; check `src/predict.py` for exact behavior.

2) Grad-CAM (`src.predict._grad_cam`)

- Grad-CAM computes a coarse localization map of the model's attention for a predicted class.
- Steps (high level):
  - Identify the last convolutional module in the model (helper `_find_last_conv_module`).
  - Run a forward pass and compute gradients of the target class w.r.t. that conv layer's activations.
  - Pool the gradients spatially to form channel weights, form a weighted sum of feature maps, apply ReLU → normalized heatmap.
- Output: a 2D float heatmap in [0,1] for the input image size.

3) Image predict pipeline (`src.predict.predict_and_save_with_box`)

- Accepts an image path, runs model inference and Grad-CAM for the top class, then:
  - Thresholds the CAM to form a binary mask, finds contours to locate object regions
  - Calculates bounding boxes (padding applied) and draws red boxes/labels on the image
  - Saves the annotated image under `outputs/<timestamp>/<original_basename>_annotated.jpg`.
  - Optionally the code can save cropped regions corresponding to detected boxes.
- Heuristics exist to handle multi-object detection by analyzing CAM peaks and per-class CAMs (see comments in `predict.py` for details and limitations).

4) Video pipeline (`src.video_predict.process_video`)

- Uses OpenCV to read frames; for every Nth frame (controlled by `frame_step`) the script:
  - Resizes and normalizes a frame for the model
  - Runs inference, computes Grad-CAM, thresholds it, extracts a bounding box if a region exists
  - Draws box + label on the frame
  - Resizes frame output based on an optional `scale` factor and writes frames to an MP4 via cv2.VideoWriter
- If the video is long, this can be time-consuming; consider setting `frame_step > 1` or scaling down.

Note: current video processing is frame-wise and uses a per-frame top-class CAM. For robust multi-object detection per-frame, port the richer multi-object heuristics used in the image pipeline.

5) Dashboard (`dashboard.py` - Streamlit)

- Sidebar:
  - Uploaders for images and videos
  - Processing options for video (`scale`, `frame_step`)
- Image flow:
  - Upload an image → saved temporarily → `predict_and_save_with_box` is called → annotated image is shown
  - The dashboard computes class probabilities and renders a probability bar chart below the annotated image
- Video flow:
  - Upload a video (saved to a temporary file) → user sets options → clicks `Process video`
  - `process_video` runs; when it completes the app shows a success message and a direct `Download annotated video` button (no inline preview) to keep the UI simple

Implementation notes:
- The dashboard shows environment diagnostics (Python executable, torch import status) to help avoid interpreter/venv mismatches.
- For better browser playback compatibility, annotated MP4 encoding/codes may need H.264; a fallback transcode step is included if embedding fails.

----

How to run (development)

1) Create and activate a Python virtual environment in the project root. Example (PowerShell):

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

2) Start the Streamlit dashboard (while venv is active):

```powershell
.\.venv\Scripts\python.exe -m streamlit run dashboard.py
# Open http://localhost:8501 (or 8502 if 8501 is used)
```

3) Image CLI prediction (optional):

```powershell
.\.venv\Scripts\python.exe -m src.predict --image path\to\image.jpg
```

4) Video processing CLI (optional):

```powershell
.\.venv\Scripts\python.exe -m src.video_predict --video_path path\to\video.mp4 --scale 1.0 --frame_step 1
```

5) Print model architecture (helper script):

```powershell
.\.venv\Scripts\python.exe scripts\print_model_summary.py
```

----

Training & evaluation

- Training script: `src/train.py` — trains MobileNetV2 on `data/animals/train`, optionally validates on `data/animals/val`, and saves the best weights to `models/best_model.pt`.
- Evaluation: if training saved a `models/metrics.npy` or similar history file, it will contain per-epoch statistics (accuracy, loss). The dashboard looks for `models/metrics.npy` to show epoch-vs-accuracy/loss plots.
- If metrics were not saved, run an evaluation pass using the validation set to get a single accuracy/loss snapshot. A helper script `scripts/evaluate_validation.py` or similar can be used; otherwise `src/predict.py` / `src/dataset.py` have code to iterate over the validation images programmatically and compute accuracy/loss.

Suggested evaluation steps (quick accuracy snapshot):

1. Ensure `models/best_model.pt` exists.
2. Run a small Python script that loads `models/best_model.pt`, creates a DataLoader for `data/animals/val`, computes logits for the validation set and reports:
   - Overall accuracy (correct / total)
   - Average loss (e.g., CrossEntropy)

If you want, I can run this evaluation now and save a plot in `outputs/`.

----

Outputs & artifact paths

- Annotated images & videos are saved under `outputs/<YYYYMMDD_HHMMSS>/` with clear names:
  - `<image>_annotated.jpg`
  - `<video>_annotated.mp4`
- Model weights: `models/best_model.pt` (not included in the remote by default to avoid large uploads)

----

Troubleshooting & notes

- If Streamlit complains `ModuleNotFoundError: No module named 'torch'` when you run the dashboard, make sure Streamlit is using the project virtual environment Python where `torch` is installed. Start Streamlit with the venv python explicitly: `.\.venv\Scripts\python.exe -m streamlit run dashboard.py`.
- MP4 playback in the browser: some generated MP4s may not embed due to codec mismatch; use `ffmpeg` to transcode to H.264 if necessary.
- Large files: GitHub rejects files >100 MB. Use Git LFS or host weight files externally, or keep `models/` ignored and provide models separately.

----

Limitations & future improvements

- Current localization uses Grad-CAM, which provides coarse heatmaps and is not an object detector. For accurate multi-object detection consider training an object detection model (YOLO, Faster R-CNN) or using a segmentation model.
- Video pipeline processes frames independently; improving temporal smoothing or tracking would reduce flickering and improve consistency.
- Dashboard improvements: add progress streaming for video processing, show per-frame detections table, or allow batch processing.

----

Contact and next steps

If you want I can:
- Run a validation evaluation now and save a small plot and a metrics file under `models/`.
- Add epoch-vs-accuracy/loss plotting into the dashboard if you provide a `models/metrics.npy` file (or allow retraining here and save metrics).
- Commit the new `README.md` and push to your GitHub remote.

Tell me which of these you want next and I'll proceed.
# Animal Species Prediction (CIFAR-10 Subset)

This is a complete, working project for **Animal Species Prediction** using a subset of the CIFAR-10 dataset (animals only: `bird`, `cat`, `deer`, `dog`, `frog`, `horse`).

The project includes:
- Data preparation script that **auto-downloads** CIFAR-10 and exports a tidy folder dataset.
- PyTorch training script (transfer learning on a small model).
- Inference script for single-image predictions.
- Reproducible environment via `requirements.txt`.
- One-click runner scripts (`run.sh` / `run.bat`).

> Note: The dataset is **downloaded automatically** on first run (CIFAR-10, ~170 MB). It will be exported into `data/animals/` with a train/val split. This keeps the zip small and installation easy.

---

## Quickstart (Windows, PowerShell)

```powershell
python -m venv .venv
.\\.venv\\Scripts\\activate
pip install -r requirements.txt
python scripts/prepare_dataset.py
python src/train.py
python src/predict.py --image_path sample_images/dog.jpg
```

## Quickstart (Linux/macOS)

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python scripts/prepare_dataset.py
python src/train.py
python src/predict.py --image_path sample_images/dog.jpg
```

---

## Project Structure

```
animal-species-prediction/
├── data/
│   └── animals/
│       ├── train/
│       └── val/
├── models/
├── sample_images/
├── scripts/
│   └── prepare_dataset.py
├── src/
│   ├── dataset.py
│   ├── train.py
│   └── predict.py
├── requirements.txt
├── run.sh
├── run.bat
└── README.md
```

---

## Classes

We keep these **6 animal classes** from CIFAR-10:

```
bird, cat, deer, dog, frog, horse
```

---

## Tips

- First training run may take a while as it compiles code and caches datasets.
- If you **don’t have a GPU**, it will automatically use CPU.
- You can change hyperparameters (epochs, batch size, lr) via CLI args in `train.py`.

---

## License

- Code: MIT
- Dataset: CIFAR-10 (per its terms). Downloaded via `torchvision`.
